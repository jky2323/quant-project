import pandas as pd
import numpy as np
import glob
import os

def load_and_combine_data(base_dir='./data', file_pattern='snapshot_sym*_date*_*.csv'):
    """
    åŠ è½½æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ç¬¦åˆå‘½åè§„åˆ™çš„CSVæ–‡ä»¶ï¼Œå¹¶åˆå¹¶æˆä¸€ä¸ªå¤§çš„DataFrameã€‚

    Args:
        base_dir (str): å­˜å‚¨CSVæ–‡ä»¶çš„æ ¹ç›®å½•ã€‚
        file_pattern (str): åŒ¹é…æ–‡ä»¶åçš„æ¨¡å¼ã€‚

    Returns:
        pd.DataFrame: åˆå¹¶åçš„æ‰€æœ‰æ•°æ®ã€‚
    """
    # ä½¿ç”¨globåŒ¹é…æ‰€æœ‰æ–‡ä»¶
    search_path = os.path.join(base_dir, file_pattern)
    all_files = glob.glob(search_path)

    if not all_files:
        print(f"âš ï¸ Error: No files found in directory {base_dir} matching {file_pattern}. Please check the path.")
        return pd.DataFrame()

    list_df = []
    
    for filename in all_files:
        try:
            df = pd.read_csv(filename)
            
            # --- æå–æ ‡è¯†ç¬¦ ---
            # å‡è®¾æ–‡ä»¶åä¸º snapshot_sym01_date20251201_am.csv
            parts = os.path.basename(filename).split('_')
            sym = parts[1].replace('sym', '')
            date = parts[2].replace('date', '')
            
            # åœ¨æŸäº›æ–‡ä»¶å‘½åæ ¼å¼ä¸­ï¼Œ'am/pm'å¯èƒ½åœ¨æ–‡ä»¶åæœ€å
            ampm_part = parts[-1].replace('.csv', '')
            if ampm_part not in ['am', 'pm']:
                 # å°è¯•ä»å€’æ•°ç¬¬äºŒéƒ¨åˆ†æå–
                ampm_part = parts[-2].replace('.csv', '')

            df['sym'] = sym
            df['date'] = date
            df['ampm'] = ampm_part
            
            # åˆ›å»ºå”¯ä¸€çš„äº¤æ˜“æ—¶æ®µæ ‡è¯†ç¬¦
            df['unique_id'] = df['sym'].astype(str) + '_' + df['date'].astype(str) + '_' + df['ampm']
            
            list_df.append(df)
            
        except Exception as e:
            print(f"âŒ Error reading file {filename}: {e}")
            continue

    if not list_df:
        return pd.DataFrame()
        
    full_df = pd.concat(list_df, ignore_index=True)
    print(f"âœ… Successfully loaded and combined {len(list_df)} files. Total rows: {len(full_df)}")
    return full_df


def preprocess_data(df, threshold=0.1):
    """
    æ‰§è¡Œæ•°æ®æ¸…æ´—ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹è½¬æ¢ã€ç¼ºå¤±å€¼å¡«å……ï¼Œå¹¶åˆ é™¤è¿‡å¤šç¼ºå¤±å€¼çš„äº¤æ˜“æ—¶æ®µã€‚

    Args:
        df (pd.DataFrame): åŸå§‹åˆå¹¶æ•°æ®ã€‚
        threshold (float): å¦‚æœä¸€ä¸ªäº¤æ˜“æ—¶æ®µçš„ç¼ºå¤±å€¼æ¯”ä¾‹è¶…è¿‡æ­¤é˜ˆå€¼ï¼Œåˆ™åˆ é™¤è¯¥æ—¶æ®µã€‚

    Returns:
        pd.DataFrame: æ¸…æ´—åçš„æ•°æ®ã€‚
    """
    if df.empty:
        return df

    # --- 1. æ•°æ®ç±»å‹è½¬æ¢ ---
    # ç¡®å®šéœ€è¦è½¬æ¢ä¸ºæ•°å€¼å‹çš„ç‰¹å¾åˆ—
    # æ’é™¤æ ‡è¯†ç¬¦åˆ—ï¼šdate, time, sym, ampm, unique_id
    id_cols = ['date', 'time', 'sym', 'ampm', 'unique_id']
    feature_cols = [col for col in df.columns if col not in id_cols]

    for col in feature_cols:
        # å°è¯•å°†ç‰¹å¾åˆ—è½¬æ¢ä¸ºæ•°å€¼å‹ã€‚æ— æ³•è½¬æ¢çš„é”™è¯¯å€¼å°†å˜æˆ NaN
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # --- 2. ç¼ºå¤±å€¼å¡«å…… (ffill) ---
    # æŒ‰ç…§ 'unique_id' åˆ†ç»„ï¼Œåªåœ¨åŒä¸€ä¸ªäº¤æ˜“æ—¶æ®µå†…ç”¨ä¸Šä¸€ä¸ªtickçš„å€¼å¡«å……
    # å³ï¼Œå¦‚æœ n_midprice[t] æ˜¯ NaNï¼Œç”¨ n_midprice[t-1] å¡«å……
    print("â³ Applying Forward Fill (ffill) grouped by 'unique_id'...")
    df[feature_cols] = df.groupby('unique_id')[feature_cols].ffill()
    
    # --- 3. åˆ é™¤è¿‡å¤šç¼ºå¤±å€¼çš„äº¤æ˜“æ—¶æ®µ ---
    # å¦‚æœä¸€ä¸ª unique_id ä¸‹çš„æŸåˆ—ï¼ˆä¾‹å¦‚ n_midpriceï¼‰ä»ç„¶å¤§é‡ç¼ºå¤±ï¼ˆå› ä¸ºè¯¥åˆ—çš„ç¬¬ä¸€ä¸ªå€¼å°±æ˜¯ç¼ºå¤±å€¼ï¼‰ï¼Œ
    # æˆ‘ä»¬å¯ä»¥åˆ é™¤æ•´ä¸ªäº¤æ˜“æ—¶æ®µã€‚
    
    # ä½¿ç”¨ n_midprice åˆ—ä½œä¸ºåˆ¤æ–­æ ‡å‡†
    missing_data_info = df.groupby('unique_id')['n_midprice'].apply(lambda x: x.isnull().sum() / len(x))
    
    # æ‰¾å‡ºç¼ºå¤±å€¼æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼çš„ unique_id
    ids_to_drop = missing_data_info[missing_data_info > threshold].index.tolist()

    if ids_to_drop:
        original_count = len(df)
        df = df[~df['unique_id'].isin(ids_to_drop)]
        print(f"ğŸ—‘ï¸ Dropped {len(ids_to_drop)} sessions (out of {len(missing_data_info)} total) due to >{threshold*100}% missing n_midprice.")
        print(f"   Total rows remaining: {len(df)} (Dropped {original_count - len(df)} rows)")
    else:
        print("âœ… No trading sessions dropped due to excessive missing data.")
        
    # å¯¹äºå‰©ä½™çš„ NaNï¼ˆé€šå¸¸æ˜¯æ¯ä¸ª session çš„ç¬¬ä¸€è¡Œï¼Œå› ä¸º ffill æ— æ³•å¡«å……ï¼‰ï¼Œ
    # ç”±äºé‡‘èæ•°æ®ç¬¬ä¸€è¡Œç¼ºå¤±è¾ƒéš¾å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç›´æ¥åˆ é™¤è¿™äº›è¡Œï¼Œæˆ–è€…ç”¨ 0 å¡«å……ã€‚
    # è¿™é‡Œé€‰æ‹©ç”¨ 0 å¡«å……ä»¥ä¿ç•™å°½å¯èƒ½å¤šçš„æ•°æ®ã€‚
    df[feature_cols] = df[feature_cols].fillna(0)

    print("âœ… Data cleaning complete.")
    return df

