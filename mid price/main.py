import os
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

from data_processor import load_and_combine_data, preprocess_data
from feature_engineer import create_all_features, generate_labels
import model_trainer_fixed as mt

from sklearn.metrics import f1_score, confusion_matrix, classification_report


def compute_per_class_accuracy(y_true, y_pred, classes=[0,1,2]):
    accs = {}
    for c in classes:
        mask = (y_true == c)
        if mask.sum() == 0:
            accs[c] = np.nan
        else:
            accs[c] = (y_pred[mask] == y_true[mask]).mean()
    return accs


def plot_class_accuracies(per_class_acc, overall_acc, out_dir='outputs'):
    os.makedirs(out_dir, exist_ok=True)
    # Use English labels for plots to avoid Chinese font issues
    labels = ['Down (0)', 'Flat (1)', 'Up (2)']
    vals = [per_class_acc.get(0, np.nan), per_class_acc.get(1, np.nan), per_class_acc.get(2, np.nan)]

    plt.figure(figsize=(6,4))
    sns.barplot(x=labels, y=vals, palette='viridis')
    plt.ylim(0,1)
    plt.title(f'Per-class accuracy (Overall acc={overall_acc:.4f})')
    plt.ylabel('Accuracy')
    for i, v in enumerate(vals):
        plt.text(i, 0.01 + (v if not np.isnan(v) else 0), f"{v:.3f}" if not np.isnan(v) else "n/a", ha='center')
    fname = os.path.join(out_dir, 'per_class_accuracy.png')
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()
    print(f"âœ… å·²ä¿å­˜æ¯ç±»å‡†ç¡®ç‡å›¾åƒåˆ°: {fname}")


def plot_confusion_matrix(cm, class_names=None, out_dir='outputs'):
    os.makedirs(out_dir, exist_ok=True)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    fname = os.path.join(out_dir, 'confusion_matrix.png')
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()
    print(f"âœ… å·²ä¿å­˜æ··æ·†çŸ©é˜µå›¾åƒåˆ°: {fname}")


def main(data_dir='./data', target_col='label5', num_rounds=200):
    """ä¸»è¿è¡Œæµç¨‹ï¼šåŠ è½½æ•°æ® -> é¢„å¤„ç† -> ç‰¹å¾ -> ç”Ÿæˆæ ‡ç­¾ -> è®­ç»ƒ -> è¯„ä¼° -> å¯è§†åŒ–"""
    print("ğŸš€ å¼€å§‹ä¸»æµç¨‹")

    # 1) åŠ è½½å¹¶åˆå¹¶æ•°æ®
    df = load_and_combine_data(base_dir=data_dir)
    if df.empty:
        print("é”™è¯¯ï¼šæœªåŠ è½½åˆ°æ•°æ®ï¼Œé€€å‡ºã€‚")
        return

    # 2) é¢„å¤„ç†
    df = preprocess_data(df)

    # 3) ç‰¹å¾å·¥ç¨‹
    df = create_all_features(df)

    # 4) ç”Ÿæˆæ ‡ç­¾
    df = generate_labels(df)

    # 5) å¯é€‰åœ°æŒ‰æ—¥çº¿åˆ†ç±»ï¼ˆç”Ÿæˆ daily_trend åˆ—ï¼‰
    df = mt.classify_by_daily_trend(df)

    # 6) æˆäº¤é‡ä¸å…¶ä»–ç‰¹å¾é¢„å¤„ç†
    df = mt.preprocess_volume_and_features(df)

    # 7) æŒ‰æ—¶é—´åºåˆ—åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•
    df_train, df_test = mt.split_data_time_series(df, train_ratio=0.7)

    # 8) å‡†å¤‡ X å’Œ yï¼ˆä½¿ç”¨ä¼ å…¥çš„ target_colï¼Œä¾‹å¦‚ 'label5'ï¼‰
    X_train, y_train, feature_cols = mt.prepare_X_y(df_train, target_col=target_col)
    X_test, y_test, _ = mt.prepare_X_y(df_test, target_col=target_col)

    # 9) æ ‡å‡†åŒ–
    X_train_s, X_test_s, scaler = mt.standardize_features(X_train, X_test)

    # 10) è®­ç»ƒæ¨¡å‹
    model = mt.train_xgboost_model(X_train_s, y_train, num_rounds=num_rounds)

    # 11) è¯„ä¼°å¹¶è·å–é¢„æµ‹
    eval_res = mt.evaluate_model(model, X_test_s, y_test)
    y_pred = eval_res['y_pred']
    overall_acc = eval_res['accuracy']
    cm = eval_res['confusion_matrix']

    # 12) è®¡ç®— per-class accuracy
    per_class_acc = compute_per_class_accuracy(y_test.values, y_pred, classes=[0,1,2])

    # 13) æ‰“å°å¹¶å¯è§†åŒ–
    print('\n--- è¯„ä¼°ç»“æœæ¦‚è¦ ---')
    print(f"æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.4f}")
    print('å„ç±»å‡†ç¡®ç‡:')
    for k,v in per_class_acc.items():
        print(f"  ç±» {k}: {v}")

    # F1 åˆ†æ•°
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    print(f"F1 (macro): {f1_macro:.4f}")
    print(f"F1 (weighted): {f1_weighted:.4f}")

    # æ›´è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    print('\nåˆ†ç±»æŠ¥å‘Š:\n')
    print(classification_report(y_test, y_pred, target_names=['ä¸‹è·Œ(0)','æŒå¹³(1)','ä¸Šå‡(2)']))

    # å¯è§†åŒ–å¹¶ä¿å­˜å›¾ç‰‡
    plot_class_accuracies(per_class_acc, overall_acc)
    plot_confusion_matrix(cm, class_names=['ä¸‹è·Œ(0)','æŒå¹³(1)','ä¸Šå‡(2)'])

    # Feature importance: extract and save if available (robust to model type)
    try:
        fi = mt.get_feature_importance(model, feature_cols, top_n=30)
        if fi is not None and not fi.empty:
            out_dir = 'outputs'
            os.makedirs(out_dir, exist_ok=True)
            plt.figure(figsize=(8,6))
            sns.barplot(x='importance', y='feature', data=fi.head(20))
            plt.title('Top 20 Feature Importance')
            plt.tight_layout()
            fpath = os.path.join(out_dir, 'feature_importance.png')
            plt.savefig(fpath)
            plt.close()
            print(f"âœ… Saved feature importance plot: {fpath}")
    except Exception as e:
        print(f"âš ï¸ Failed to generate feature importance: {e}")

    # Write a concise final report to outputs
    try:
        out_dir = 'outputs'
        os.makedirs(out_dir, exist_ok=True)
        report_path = os.path.join(out_dir, 'FINAL_REPORT.txt')
        with open(report_path, 'w') as rf:
            rf.write('Model Evaluation Report\n')
            rf.write('========================\n')
            rf.write(f'Overall accuracy: {overall_acc:.6f}\n')
            rf.write(f'F1 (macro): {f1_macro:.6f}\n')
            rf.write(f'F1 (weighted): {f1_weighted:.6f}\n')
            rf.write('\nPer-class accuracy:\n')
            for k, v in per_class_acc.items():
                rf.write(f' - Class {k}: {v}\n')
            rf.write('\nConfusion matrix:\n')
            rf.write(np.array2string(cm))
            rf.write('\n')
            voting_path = os.path.join('model_artifacts', 'voting_ensemble_results.pkl')
            if os.path.exists(voting_path):
                rf.write(f'Voting ensemble pickle: {voting_path}\n')
        print(f"âœ… Saved final report to: {report_path}")
    except Exception as e:
        print(f"âš ï¸ Failed to write final report: {e}")

    print("ğŸ¯ ä¸»æµç¨‹å®Œæˆ")


if __name__ == '__main__':
    # ä½ å¯ä»¥ä¿®æ”¹ target_col æˆ– num_rounds æ¥åŠ é€Ÿ/è°ƒå‚
    main(data_dir='./data', target_col='label5', num_rounds=200)
