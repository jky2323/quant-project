import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix


def classify_by_daily_trend(df):
    """
    æŒ‰æ¯æ”¯è‚¡ç¥¨çš„æ—¥çº¿èµ°åŠ¿åˆ†ç±» - ä½¿ç”¨closeä»·æ ¼è®¡ç®—æ—¥å†…å¼€æ”¶
    ç”±äºæ²¡æœ‰n_openï¼Œä½¿ç”¨æ¯æ—¥ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªcloseä»·æ ¼è¿‘ä¼¼
    """
    print("\n--- é˜¶æ®µ1: æŒ‰æ—¥çº¿èµ°åŠ¿åˆ†ç±» ---")
    
    df = df.copy()
    
    # è®¡ç®—æ¯æ—¥çš„å¼€æ”¶ä»·ï¼ˆæŒ‰sym+dateåˆ†ç»„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªä»·æ ¼ï¼‰
    daily_stats = df.groupby(['sym', 'date']).agg({
        'n_close': ['first', 'last']
    }).reset_index()
    daily_stats.columns = ['sym', 'date', 'close_open', 'close_last']
    
    daily_stats['daily_return'] = (daily_stats['close_last'] - daily_stats['close_open']) / daily_stats['close_open']
    
    # å®šä¹‰åˆ†ç±»è§„åˆ™: ä¸‹è·Œ=-1, æŒå¹³=0, ä¸Šå‡=1
    daily_stats['daily_trend'] = pd.cut(daily_stats['daily_return'], 
                                        bins=[-np.inf, -0.001, 0.001, np.inf],
                                        labels=[-1, 0, 1])
    # ä½¿ç”¨fillnaå¤„ç†NaNå€¼ï¼Œç„¶åè½¬æ¢ä¸ºint
    daily_stats['daily_trend'] = daily_stats['daily_trend'].fillna(0).astype(int)
    
    # åˆå¹¶å›åŸå§‹æ•°æ®
    df = df.merge(daily_stats[['sym', 'date', 'daily_trend']], on=['sym', 'date'], how='left')
    
    # ç»Ÿè®¡åˆ†å¸ƒ
    distribution = df['daily_trend'].value_counts().sort_index()
    print(f"âœ… æ—¥çº¿åˆ†ç±»ç»“æœ:")
    print(f"   ä¸‹è·Œ (-1): {distribution.get(-1, 0)} å¤©")
    print(f"   æŒå¹³ (0):  {distribution.get(0, 0)} å¤©")
    print(f"   ä¸Šå‡ (1):  {distribution.get(1, 0)} å¤©")
    
    return df


def preprocess_volume_and_features(df):
    """
    å¯¹æˆäº¤é‡å’Œè¡ç”Ÿç‰¹å¾åº”ç”¨logå˜æ¢ã€‚
    """
    print("\n--- é˜¶æ®µ2: ç‰¹å¾é¢„å¤„ç† ---")
    
    df = df.copy()
    
    # éœ€è¦å¯¹æ•°å˜æ¢çš„åˆ—
    log_cols = ['amount_delta', 'volume_change_pct', 'volume_ma5_ratio',
                'volume_ma10_ratio', 'volume_ma20_ratio', 'bid_volume', 'ask_volume']
    
    applied_cols = []
    for col in log_cols:
        if col in df.columns:
            df[col] = np.log1p(df[col])
            applied_cols.append(col)
    
    print(f"âœ… å¯¹{len(applied_cols)}ä¸ªæˆäº¤é‡ç›¸å…³ç‰¹å¾åº”ç”¨log(x+1)å˜æ¢")
    
    return df


def split_data_time_series(df, train_ratio=0.7):
    """
    æŒ‰æ—¶é—´åºåˆ—åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
    """
    print("\n--- é˜¶æ®µ3: æ—¶é—´åºåˆ—åˆ’åˆ† ---")
    
    # æŒ‰æ ·æœ¬æ•°é‡åˆ’åˆ†
    split_idx = int(len(df) * train_ratio)
    df_train = df.iloc[:split_idx].copy()
    df_test = df.iloc[split_idx:].copy()
    
    print(f"âœ… æ•°æ®å·²æŒ‰æ—¶é—´åºåˆ—åˆ’åˆ†")
    print(f"   è®­ç»ƒé›†: {len(df_train):,} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(df_test):,} æ ·æœ¬")
    
    return df_train, df_test


def prepare_X_y(df, target_col='label_5'):
    """
    ä»DataFrameä¸­åˆ†ç¦»ç‰¹å¾ X å’Œæ ‡ç­¾ yã€‚
    """
    print(f"\n--- é˜¶æ®µ4: æ•°æ®å‡†å¤‡ ---")
    
    # å®šä¹‰éœ€è¦æ’é™¤çš„åˆ—
    LABEL_COLS = [col for col in df.columns if col.startswith('label')]
    ID_COLS = ['date', 'time', 'sym', 'ampm', 'unique_id', 'daily_trend', 'n_close', 'n_midprice', 'amount_delta']
    
    # ä¸¢å¼ƒæ ‡ç­¾ä¸ºNaNçš„æ ·æœ¬
    df_clean = df.dropna(subset=[target_col]).copy()
    print(f"âœ… ä¸¢å¼ƒNaNæ ‡ç­¾: {len(df) - len(df_clean)} è¡Œ â†’ {len(df_clean):,} è¡Œ")
    
    # ç‰¹å¾åˆ—
    feature_cols = [col for col in df_clean.columns 
                   if col not in LABEL_COLS and col not in ID_COLS]
    
    X = df_clean[feature_cols].copy()
    y = df_clean[target_col].copy()
    
    # å¤„ç†NaNå’Œæ— ç©·å¤§
    X = X.fillna(0)
    X = X.replace([np.inf, -np.inf], 0)
    
    # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°
    y = y.astype(int)
    
    print(f"âœ… ç‰¹å¾å‡†å¤‡å®Œæˆ: {len(feature_cols)} ä¸ªç‰¹å¾, {len(X):,} ä¸ªæ ·æœ¬")
    
    return X, y, feature_cols


def standardize_features(X_train, X_test):
    """
    ä½¿ç”¨StandardScaleræ ‡å‡†åŒ–ç‰¹å¾ã€‚
    """
    print(f"\n--- æ ‡å‡†åŒ–ç‰¹å¾ ---")
    
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns
    )
    
    print(f"âœ… ç‰¹å¾å·²æ ‡å‡†åŒ–")
    print(f"   è®­ç»ƒé›†: shape={X_train_scaled.shape}")
    print(f"   æµ‹è¯•é›†: shape={X_test_scaled.shape}")
    
    return X_train_scaled, X_test_scaled, scaler


def train_xgboost_model(X_train, y_train, num_rounds=500):
    """
    ä½¿ç”¨XGBoostè®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹ã€‚
    """
    print(f"\n--- é˜¶æ®µ5: XGBoostæ¨¡å‹è®­ç»ƒ ---")
    
    params = {
        'objective': 'multi:softmax',
        'num_class': 3,
        'eval_metric': 'mlogloss',
        'eta': 0.1,
        'max_depth': 6,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'seed': 42,
        'nthread': -1
    }
    
    dtrain = xgb.DMatrix(X_train, label=y_train)
    
    print(f"â³ å¼€å§‹è®­ç»ƒXGBoost (è½®æ•°={num_rounds})...")
    model = xgb.train(params, dtrain, num_rounds)
    print(f"âœ… è®­ç»ƒå®Œæˆ")
    
    return model


def evaluate_model(model, X_test, y_test):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    """
    print(f"\n--- é˜¶æ®µ6: æ¨¡å‹è¯„ä¼° ---")
    
    dtest = xgb.DMatrix(X_test)
    y_pred = model.predict(dtest).astype(int)
    
    acc = accuracy_score(y_test, y_pred)
    
    print(f"\nâœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}")
    print(f"\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, 
                              target_names=['ä¸‹è·Œ(-1)', 'æŒå¹³(0)', 'ä¸Šå‡(1)']))
    print(f"\nğŸ”² æ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(y_test, y_pred))
    
    return {
        'accuracy': acc,
        'y_pred': y_pred,
        'confusion_matrix': confusion_matrix(y_test, y_pred)
    }


def get_feature_importance(model, feature_cols, top_n=20):
    """
    Robustly extract feature importance from either an XGBoost Booster or
    an sklearn-like estimator with `feature_importances_`.

    Returns a pandas DataFrame with columns ['feature','importance'] sorted
    by importance descending. If extraction fails, returns empty DataFrame.
    """
    import pandas as pd
    try:
        # Case 1: xgboost.Booster (returned by xgb.train)
        if hasattr(model, 'get_score'):
            importance_dict = model.get_score(importance_type='weight')
            rows = []
            for k, v in importance_dict.items():
                # keys might be 'f0','f1' or actual feature names
                if isinstance(k, str) and k.startswith('f'):
                    try:
                        idx = int(k[1:])
                        name = feature_cols[idx] if idx < len(feature_cols) else k
                    except Exception:
                        name = k
                else:
                    name = k
                rows.append({'feature': name, 'importance': v})
            if not rows:
                return pd.DataFrame()
            df = pd.DataFrame(rows).sort_values('importance', ascending=False)
            return df

        # Case 2: sklearn-like estimator with feature_importances_
        if hasattr(model, 'feature_importances_'):
            import numpy as np
            imp = np.array(model.feature_importances_)
            rows = [{'feature': feature_cols[i], 'importance': float(imp[i])}
                    for i in range(min(len(feature_cols), len(imp)))]
            df = pd.DataFrame(rows).sort_values('importance', ascending=False)
            return df

    except Exception as e:
        # Don't raise â€” return empty DataFrame to keep pipeline robust
        print(f"Warning: failed to extract feature importance: {e}")

    return pd.DataFrame()
